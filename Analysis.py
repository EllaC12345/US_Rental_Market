# -*- coding: utf-8 -*-
"""Project4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/EllaC12345/Machine-Learning/blob/main/Project4.ipynb
"""
#%%
import pandas as pd
from pathlib import Path
from sklearn.cluster import KMeans, AgglomerativeClustering,Birch
#!pip install matplotlib-venn
from sklearn.preprocessing import StandardScaler, OneHotEncoder
import seaborn as sns
#!pip install hvplot
#!pip install hvplot
#!pip install pycaret

#from bokeh.io import output_notebook
import plotly.express as px




#%%
#pip install --upgrade pycaret

#!pip uninstall scipy

import numpy as np
import pandas as pd
import plotly.express as px
import matplotlib.pyplot as plt
import holoviews as hv
#!pip install pycaret



# Clustering & PCA
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans

# Machine Learning
import pycaret.classification as clf
from xgboost import plot_importance

#%%
## Read the csv files
file_name = '/Users/ellandalla/Desktop/Portfolio/Unsupervised/apartments_for_rent_classified_100K.csv'
rent_df = pd.read_csv(file_name, sep=";", encoding="latin-1")

rent_df.head()

## Create distinct  appartment types based on the number of bedrooms and bathrooms

rent_df["apartmemt Category"] = rent_df['bedrooms'].astype(str) + ' BR ' + rent_df['bathrooms'].astype(str) + ' BA'
rent_df

## Summary of columns in the dataset
rent_df.columns

rent_df.describe(include='all')


## converting datatype for certain fields
columns_to_convert = ['id',  'price',  'square_feet']
rent_df[columns_to_convert] = rent_df[columns_to_convert].values.astype(np.float64)


rent_df.dtypes

## Data exploration
#%%


rent_df.info()

numerical_columns = rent_df.select_dtypes(include=[np.number]).columns.tolist()
numerical_columns.remove('time')

category_columns = rent_df.select_dtypes(include = ['object']).columns.tolist()
category_columns
category_data_df = rent_df[category_columns]

numerical_data_df = rent_df[numerical_columns]



#Category data profiling
def numeric_profile_data(data):
    """Panda Profiling Function

    Args:
        data (DataFrame): A data frame to profile

    Returns:
        DataFrame: A data frame with profiled data
    """
    numeric_data = data.select_dtypes(include=[np.number])
    profile_df = pd.concat([
        pd.DataFrame({'Dtype': numeric_data.dtypes}, index=numeric_data.columns),
        # Counts
        numeric_data.count().rename("Count"),
        numeric_data.isnull().sum().rename("NA Count"),
        numeric_data.nunique().rename("Count Unique"),
        # Stats
        numeric_data.min().rename("Min"),
        numeric_data.max().rename("Max"),
        numeric_data.mean().rename("Mean"),
        numeric_data.median().rename("Median"),
        numeric_data.mode().iloc[0].rename("Mode"),
    ], axis=1)
    return profile_df
  


def category_profile_data(data):
    """Panda Profiling Function

    Args:
        data (DataFrame): A data frame to profile

    Returns:
        DataFrame: A data frame with profiled data
    """
    
    data = data.select_dtypes(include=['object'])
    profile_df = pd.concat([
        pd.DataFrame({'Dtype': data.dtypes}, index=data.columns),
            # Counts
            data.count().rename("Count"),
            data.isnull().sum().rename("NA Count"),
            data.nunique().rename("Count Unique"),
            # Stats
            data.mode().iloc[0].rename("Mode"),
        ],axis=1)
    return profile_df
    
    
numeric_profile_data(rent_df)
category_profile_data(rent_df)






## Data Cleaning
#%%
unique_amenities= list(rent_df['amenities'].unique())

print (unique_amenities)

amenities_df = rent_df['amenities'].str.split(',').explode()
amenities_df.head()

# Count the occurrences of each amenity
amenities_counts = amenities_df.value_counts()

# Display the result
print(amenities_counts)


# Remove leading and trailing spaces in the 'amenities' column
rent_df['amenities'] = rent_df['amenities'].str.strip()

# Split the 'amenities' column and explode the lists into separate rows
amenities_df = rent_df['amenities'].str.split(',').explode()

# Count the occurrences of each amenity
amenities_counts = amenities_df.value_counts()

# Display the result
print(amenities_counts)

## create categories of amenities to incorporate them into the analysis
outdoor = ['Patio/Deck', 'Clubhouse', 'Playground']
sports = ['Pool','Gym', 'Tennis', 'Basketball', 'Golf']
luxury = ['Fireplace', 'Wood Floors', 'View', 'Doorman', 'Luxury', 'Hot Tub']
convenience = ['Parking', 'Garbage Disposal', 'Washer Dryer', 'AC', 'Elevator', 'Dishwasher', 'Storage', 'Gated', 'Refrigerator','Cable or Satellite','Internet Access']

# Convert the list to a regular expression pattern
pattern = '|'.join(outdoor)
pattern2 = '|'.join(sports)
pattern3 = '|'.join(luxury)
pattern4 = '|'.join(convenience)

# Count the number of occurrences of items in the outdoor list in the 'amenities' column
rent_df['outdoor_count'] = rent_df['amenities'].str.count(pattern)
rent_df['sports_count'] = rent_df['amenities'].str.count(pattern2)
rent_df['luxury_count'] = rent_df['amenities'].str.count(pattern3)
rent_df['convenience_count'] = rent_df['amenities'].str.count(pattern4)

# Count the number of occurrences of items in the outdoor list in the 'amenities' column
#lst = ['convenience']
#rent_df = rent_df.drop(lst, axis=1)

# Display the DataFrame with the new columns
rent_df.head()


## converting weekly rent prices into monthly
weekly = rent_df[rent_df['price_type'] == "Weekly"]
rent_df.loc[rent_df['price_type'] == 'Weekly', 'price'] *= 4


## droping incomplete rows
rent_df = rent_df[rent_df['price_type']!= 'Monthly|Weekly' ]
weekly = rent_df[rent_df['price_type'] == "Weekly"]
weekly

rent_df['price_type'].unique()



## Preprocessing #1 dropping unnecessary column for modeling
lst = ['category', 'title', 'body', 'currency','price_display','price_type','source','time', 'amenities', 'bathrooms', 'bedrooms','address' ]
rent_df = rent_df.drop(lst, axis=1)

rent_df.head()
rent_df.info()

## Managing Missing Values
#%%


# categorical columns
category_columns = rent_df.select_dtypes(include = ['object']).columns.tolist()
numerical_columns = rent_df.select_dtypes(include=[np.number]).columns.tolist()
category_columns

# Columns with low missing values are deleted
columns_to_delete=['latitude', 'longitude', 'cityname', 'state']

rent_df = rent_df.dropna(subset=columns_to_delete)


columns_with_missing_values_cat = [column for column in category_columns if rent_df[column].isnull().any()]
columns_with_missing_values_num = [column for column in numerical_columns if rent_df[column].isnull().any()]


# pets_allowed has the most missing values, we will treat the lack of specification as a "yes" to pets,
# as it is more likely that the property owner would allow pets than not. missing amenities will be replaced with most frequent value

imputer = SimpleImputer(strategy='most_frequent')




# Apply imputer to columns with missing values
rent_df[columns_with_missing_values_cat] = imputer.fit_transform(rent_df[columns_with_missing_values_cat])
rent_df

rent_df[columns_with_missing_values_num] = imputer.fit_transform(rent_df[columns_with_missing_values_num])

rent_df


numeric_profile_data(rent_df)
category_profile_data(rent_df)


fee = rent_df['fee'].unique()
fee

#%%
## encoding the Has_photo type
has_photo = rent_df['has_photo'].unique()
has_photo

## encoding the photo type
def encode_photo(has_photo):
    """
    This function encodes photo by setting yes as 1 and no as 0.
    """
    if has_photo == "yes" or  has_photo == 'Thumbnail':
      return 1
    else:
        return 0

# Call the encode_fee function on the fee column
rent_df["has_photo"] = rent_df["has_photo"].apply(encode_photo)

# Review the DataFrame
rent_df.tail()

## encoding the fee type
def encode_fee(fee):
    """
    This function encodes fee status by setting yes as 1 and no as 0.
    """
    if fee == "yes":
        return 1
    else:
        return 0

# Call the encode_fee function on the fee column
rent_df["fee"] = rent_df["fee"].apply(encode_fee)

# Review the DataFrame
rent_df.head()

## encoding the pets_allowed type
pets_allowed = rent_df['pets_allowed'].unique()
pets_allowed

## encoding the pet type
def encode_pet(pets_allowed):
    """
    This function encodes pet ownership status by setting yes as 1 and no as 0.
    """
    if pets_allowed == "Cats,Dogs" or pets_allowed == "Cats" or pets_allowed == "Dogs" :
        return 1
    else:
        return 0

# Call the encode_fee function on the fee column
rent_df["pets_allowed"] = rent_df["pets_allowed"].apply(encode_pet)

# Review the DataFrame
rent_df.tail()

#removing unnecessary columns
lst= ['address']
rent_df = rent_df.drop(lst, axis=1)
rent_df.head()

rent_df.head()


#Transforming "apartment Category" column
#%%
## Transform Apartment categories column with OneHotEncoding

# Initialize the OneHotEncoder with the desired dtype
encoder = OneHotEncoder(dtype=np.uint8, sparse_output=False)

# Reshape the column to a 2D array
np_array = rent_df['apartmemt Category'].values.reshape(-1, 1)

# Fit and transform the OneHotEncoder
X_encoded = encoder.fit_transform(np_array )

# Create a DataFrame from the encoded array with appropriate column names
cat_df = pd.DataFrame(X_encoded, columns= encoder.get_feature_names_out(['apartment Category']))
cat_df

# Concatenate the df_shopping_transformed and the card_dummies DataFrames
feature_df = pd.concat([rent_df.reset_index(drop=True), cat_df.reset_index(drop=True)], axis=1)

# Drop the original education column
feature_df = feature_df.drop(columns=['apartmemt Category', 'cityname', 'state', 'id'])

# Display the DataFrame
feature_df.head()



#%%

feature_df_scaled = StandardScaler().fit_transform(feature_df[["price", "square_feet", "latitude", "longitude"]])

# Review the scaled data
feature_df_scaled

## Create a dataframe of the scaled data
feature_df_scaled = pd.DataFrame(feature_df_scaled, columns=["price", "square_feet", "latitude", "longitude"])

## replace the original data with columns of information from the scaled Data
feature_df['price'] = feature_df_scaled["price"]
feature_df["square_feet"] = feature_df_scaled["square_feet"]
feature_df["latitude"] = feature_df_scaled["latitude"]
feature_df["longitude"] = feature_df_scaled["longitude"]

feature_df




#Finding K using the Elbow Method
#%%

# importing the KMeans module from SKLEArn
from sklearn.cluster import KMeans
from sklearn import __version__ as sklearn_version
from distutils.version import LooseVersion
from sklearn.cluster import KMeans

#create a list to store inertia values of k
inertia = []
k = list(range(1,11))

#Create a for-loop where each value of k is evaluated using the K-means algorithm
# Fit the model using the service_ratings DataFrame
# Append the value of the computed inertia from the `inertia_` attribute of the KMeans model instance
for i in k:
  k_model = KMeans(n_clusters=i, random_state =0)
  k_model.fit(feature_df)
  inertia.append(k_model.inertia_)

## define the dataframe to hold the values for k and the corresponding inertia
elbow_data = {'k': k, "inertia": inertia}
df_elbow = pd.DataFrame(elbow_data)
df_elbow.head()

## Plot the dataFrame
plt.plot(df_elbow['k'], df_elbow['inertia'])
plt.title("Elbow Curve")
plt.xlabel("k")
plt.ylabel('inertia')
plt.show()

## Using KMeans to cluster the data
# Define the model with 2 clusters
model = KMeans(n_clusters=2, random_state=0)

# Fit the model
model.fit(feature_df)

# Make predictions
k_2 = model.predict(feature_df)

# Create a copy of the preprocessed data
feature_predictions_df = feature_df.copy()

# Add a class column with the labels
feature_predictions_df['rent_segments'] = k_2

feature_predictions_df.head()

# Plot the clusters


#sns.scatterplot(x="square_feet", y = "price", data = feature_predictions_df, hue = "rent_segments")

"""Reperform the Clustering techniques using Principal Components Analysis (PCA).
In this section I will reduce the dimensionality of the transformed Feature DataFrame to 3 principal components
"""
#%%

## Reperforming Clustering using PCA method
# Instantiate the PCA instance and declare the number of PCA variables
pca=PCA(n_components=3)

## Fit the PCA model on the transformed feature DataFrame
feature_pca = pca.fit_transform(feature_df)

feature_pca

## Using the explained_variance_ratio_ function from PCA, calculate the percentage of the total variance that is captured by the two PCA variables
# Calculate the PCA explained variance ratio
pca.explained_variance_ratio_

"""About 61% of total variance is condensed into the **3** PCA variables"""

# Creating the PCA DataFrame
feature_pca_df = pd.DataFrame(
    feature_pca,
    columns=["PCA1", "PCA2", "PCA3" ]
)

# Review the PCA DataFrame
feature_pca_df.head()

## reuse the elbow method to determine the optimal value of k using feature_pca_df
# Create a a list to store inertia values and the values of k
inertia = []
k = list(range(1, 11))

# Create a for-loop where each value of k is evaluated using the K-means algorithm
# Fit the model using the service_ratings DataFrame
# Append the value of the computed inertia from the `inertia_` attribute of the KMeans model instance
for i in k:
    k_model = KMeans(n_clusters=i, random_state=0)
    k_model.fit(feature_pca_df)
    inertia.append(k_model.inertia_)

# Define a DataFrame to hold the values for k and the corresponding inertia
elbow_data = {"k": k, "inertia": inertia}

# Create the DataFrame from the elbow data
df_elbow = pd.DataFrame(elbow_data)

# Review the DataFrame
df_elbow.head()

# plot the elbow curve for pca_dataframe

plt.plot(df_elbow['k'], df_elbow['inertia'])
plt.title("Elbow Curve")
plt.xlabel("k")
plt.ylabel('inertia')
plt.show()


"""using the PCA methodology the ideal amount of clusters remains at 2"""

## Segment the `feature_pca_df`  DataFrame using the K-means algorithm.
# Define the model Kmeans model using the optimal value of k for the number of clusters.
model = KMeans(n_clusters=2, random_state=0)

# Fit the model
model.fit(feature_pca_df)

# Make predictions
k_2 = model.predict(feature_pca_df)

# Create a copy of the feature_pca_df DataFrame
feature_pca_predictions_df = feature_pca_df.copy()

# Add a class column with the labels
feature_pca_predictions_df["rent_segments"] = k_2

#%%
## Plot the segments
import os
import holoviews as hv

"""os.environ['HV_DOC_HTML'] = 'true'
hv.extension('bokeh')

hv.Curve([1, 2, 3])
output_notebook()"""

fig =px.scatter_3d(
    feature_pca_predictions_df,
    x = "PCA1",
    y = "PCA2",
    z = "PCA3",
    color = "rent_segments",
    template = "plotly_dark",
    title = "PCA Space",
    width = 800,
    height = 800
).update_traces(
    marker={
        'size': 3,
        'line': {
            'width': 1,
            'color': "white"
        }
    }
)
fig.show()

"""**Conclusion:**
Although the rentals are still segmented in the same amount of clusters. The PCA method provides a clearer distinction bewtween elements in each segment.
"""



#%%
#XGBoost to improve the clustering techniques
## Leverage Machine Learning Library XGBoost to improve the clustering technique

## set up and configure the environment for training and evaluating the model
from pycaret.clustering import *

#s = setup(data, normalize = True, ignore_features = ['CUST_ID'], session_id = 145, silent = True)
clf.setup(
    data=feature_pca_predictions_df,
    target='rent_segments',
)

model_xgb = clf.create_model(
    'xgboost',
    cross_validation= False,
    verbose=True
)

df_predictions = clf.predict_model(
        model_xgb,
        data=feature_pca_predictions_df,
        raw_score=False
    )

df_predictions

#Feature Importance

#%%
# 5.0 FEATURE IMPORTANCE (TOP FEATURES) -----
#  * BENEFIT OF MACHINE LEARNING
try:
    plot_importance(model_xgb[len(model_xgb)-1])
except:
    plot_importance(model_xgb)



# replot the segments
sns.scatterplot(x="PCA2", y = "PCA3", data = feature_pca_predictions_df, hue = "rent_segments")

# reperform using feature_prediction model

clf.setup(
    data=feature_predictions_df,
    target='rent_segments',
)

model_xgb = clf.create_model(
    'xgboost',
    cross_validation= False,
    verbose=True
)

df_predictions = clf.predict_model(
        model_xgb,
        data=feature_predictions_df,
        raw_score=False
    )

df_predictions

#%%
# 5.0 FEATURE IMPORTANCE (TOP FEATURES) -----
#  * BENEFIT OF MACHINE LEARNING using feature dataframe is that we can see the most important features in the model and draw insights from each cluster.
try:
    plot_importance(model_xgb[len(model_xgb)-1])
except:
    plot_importance(model_xgb)

## copy the prediction dataframe for further analysis. replace scaled numerical columns with original data
analysis_df = df_predictions.copy()
analysis_df = analysis_df.reset_index(drop=True)
rent_df = rent_df.reset_index(drop=True)
columns_to_replace = ['latitude', 'square_feet','price','longitude', 'sports_count', 'outdoor_count', 'luxury_count', 'convenience_count']
analysis_df[columns_to_replace]= rent_df[columns_to_replace]
analysis_df.dropna(inplace=True)

## Segment Analysis cluster insights Cluster0
cluster_0 = analysis_df[analysis_df['prediction_label']== 0]
median_rent_price = cluster_0['price'].median()
print(median_rent_price)
average_rent_price = cluster_0['price'].mean()
print(median_rent_price)
median_sqft = cluster_0['square_feet'].median()
print(median_sqft)
sports_count = cluster_0['sports_count'].median()
print(sports_count)
outdoor_count = cluster_0['outdoor_count'].median()
print(outdoor_count)
convenience_count = cluster_0['convenience_count'].median()
print(convenience_count)
luxury_count = cluster_0['luxury_count'].median()
print(luxury_count)

cluster_0_df = cluster_0

## Segment Analysis cluster insights Cluster1
cluster_1 = analysis_df[analysis_df['prediction_label']== 1]
average_rent_price = cluster_1['price'].mean()
print(average_rent_price)
median_rent_price = cluster_1['price'].median()
print(median_rent_price)
median_sqft = cluster_1['square_feet'].median()
print(median_sqft)
sports_count = cluster_1['sports_count'].median()
print(sports_count)
outdoor_count = cluster_1['outdoor_count'].median()
print(outdoor_count)
convenience_count = cluster_1['convenience_count'].median()
print(convenience_count)
luxury_count = cluster_1['luxury_count'].median()
print(luxury_count)


#pip install --upgrade scipy

#%%
##Keeping essential columns for analysis
snip_df = analysis_df[["latitude", "longitude", "prediction_label", "price","square_feet", 'sports_count', 'outdoor_count', 'luxury_count', 'convenience_count']]
snip_df[['cityname', 'state']] = rent_df[['cityname', 'state']]
snip_df


snip_df.to_json('/Users/ellandalla/Desktop/Portfolio/Unsupervised/data.json', orient='records')
snip_df.to_csv('/Users/ellandalla/Desktop/Portfolio/Unsupervised/data.csv')
#%%



numeric_profile_data(analysis_df)
category_profile_data(analysis_df)



#!pip install holoviews
#!pip install cartopy
#!pip install geoviews
import holoviews as hv
from holoviews import opts
import hvplot.pandas

import geoviews as gv
#import geoviews.tile_sources as gvts
hv.extension("bokeh")
import cartopy.crs as ccrs

# Define the color mapping
color_mapping = {'0': 'blue', '1': 'orange'}

map_plot = snip_df.hvplot.points(
    "longitude",
    "latitude",
    geo = True,
    tiles = "OSM",
    frame_width = 1000,
    frame_height = 800,
    size = "(square_feet)",
    color = "prediction_label",
    cmap=color_mapping,
    tools=['box_zoom'],
    hover_cols=["cityname", "state"],
)
# Display the plot in Colab
hv.output(notebook='bokeh')
map_plot

# Display the HTML file
#from IPython.display import HTML
#HTML(filename='map_plot.html')

#%%
#!pip install sqlalchemy pandas
from sqlalchemy import create_engine
import sqlite3
import pandas as pd

from sqlalchemy import create_engine, Column, Integer, String, Float, Text
from google.colab import files

# Define connection and the cursor

connection = sqlite3.connect('rentals.db')
cursor = connection.cursor()

# Drop the existing table if it exists
cursor.execute("DROP TABLE IF EXISTS rentals")





# %%
